{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cba0bd4-06ba-4e71-9a4b-b93a7c09ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blpapi\n",
    "from blpapi import SessionOptions, Session\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from xbbg import blp\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import Haver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c80820f-a5ac-4a7a-abbd-d6a4541da060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94de252e-c932-489f-af51-c6a53866d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index_to_stationary:\n",
    "    # the point of this class is to get data from bloomberg and if that data is not stationary convert it to the 6 month over 6 month change. From there we convert them to z scores. \n",
    "    def __init__(self, bbg_ticker):\n",
    "        #this section allows me to maintian the features throughout the class and instantly lets me pull these features into other classes later wihtout ever defining it. \n",
    "        self.index_value = blp.bdh(tickers=bbg_ticker, flds=['PX_LAST'],start_date='1970-01-01',Per='D', Fill='P', Days='A')\n",
    "        self.ticker = bbg_ticker\n",
    "\n",
    "    def invert_index_value(self):\n",
    "        #this sections isto invert an inde to align directional to toehr indicies. A decline in unemployment should be positive for the employment inde and then needs to be inverted\n",
    "        self.index_value = self.index_value*-1\n",
    "    \n",
    "    def mean_revert_trans(self):\n",
    "        # This mean revesion is the idea that we need our data to be stationary, so we make the data the 6 month change from data 6 months prior. SImilar to year over year, just shorter. \n",
    "        self.index_value.index = pd.to_datetime(self.index_value.index) #convert data index to be time based\n",
    "        df_daily = self.index_value[self.ticker].copy()\n",
    "        df_daily_copy =df_daily.copy()\n",
    "        df_daily_copy=df_daily_copy.shift(freq= DateOffset(months=6)) #move data back 6 months\n",
    "        df_daily_copy=df_daily_copy.rename(columns={'PX_LAST': '6_months_ago'})\n",
    "        df_daily = pd.merge(df_daily,df_daily_copy, left_index=True, right_index=True, how = 'outer') # this mergers allows me to have a data frame with today's data and 6 months ago to carry out the change operation\n",
    "        date_before = pd.Timestamp('today')\n",
    "        df_daily1 = df_daily[df_daily.index <= date_before]\n",
    "        df_daily1 = df_daily1[~df_daily1.index.duplicated(keep='last')] #sometimes the procedure creates duplicates\n",
    "        df_daily1 = df_daily1.fillna(method='ffill') #this fills in any weekend gaps\n",
    "        # Calculate the change over the last 6 months\n",
    "        df_daily1['6_month_change'] = ((df_daily1['PX_LAST'] - df_daily1['6_months_ago'])/df_daily1['6_months_ago'])\n",
    "\n",
    "        return df_daily1\n",
    "\n",
    "    def z_scores(self):\n",
    "        #here we simply calcualte the z score\n",
    "        df_daily = self.mean_revert_trans().copy()\n",
    "        df_daily['Zscore'] = (df_daily['6_month_change'] - df_daily['6_month_change'].mean()) / df_daily['6_month_change'].std()\n",
    "        df_daily = df_daily[~df_daily.index.duplicated(keep='first')]\n",
    "        zscore_df = df_daily.drop(['PX_LAST',\"6_month_change\",'6_months_ago'], axis=1)\n",
    "        #df_daily['6_month_change'].apply(stats.zscore)\n",
    "        # Manually calculating Z-scores: z_scores_manual = (df - df.mean()) / df.std()\n",
    "        return zscore_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4483eea6-b39f-4ca6-bb98-4e8dab265397",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datapoints_to_stationary:\n",
    "    #this class is an expansion on the prior class, it allows for the conversation to stationary and then to z scores for bloomberg data that is not just the last price of an index.\n",
    "    #Used mainly for profits and market bubble index\n",
    "    def __init__(self, bbg_ticker,field):\n",
    "        #this section allows me to maintian the features throughout the class and instantly lets me pull these features into other classes later wihtout ever defining it. \n",
    "        self.index_value = blp.bdh(tickers=bbg_ticker, flds=field,start_date='1970-01-01',Per='D', Fill='P', Days='A')\n",
    "        self.ticker = bbg_ticker\n",
    "        self.field = field\n",
    "\n",
    "    def mean_revert_trans(self):\n",
    "        # Calculate the value from 6 months ago (daily data)\n",
    "        self.index_value.index = pd.to_datetime(self.index_value.index)\n",
    "        df_daily = self.index_value[self.ticker].copy()\n",
    "        df_daily_copy =df_daily.copy()\n",
    "        df_daily_copy=df_daily_copy.shift(freq= DateOffset(months=6))\n",
    "        df_daily_copy=df_daily_copy.rename(columns={self.field: '6_months_ago'})\n",
    "        df_daily = pd.merge(df_daily,df_daily_copy, left_index=True, right_index=True, how = 'outer')\n",
    "        date_before = pd.Timestamp('today')\n",
    "        df_daily1 = df_daily[df_daily.index <= date_before]\n",
    "        df_daily1 = df_daily1[~df_daily1.index.duplicated(keep='first')]\n",
    "        df_daily1 = df_daily1.fillna(method='ffill')\n",
    "        # Calculate the change over the last 6 months\n",
    "        df_daily1['6_month_change'] = ((df_daily1[self.field] - df_daily1['6_months_ago'])/df_daily1['6_months_ago'])\n",
    "\n",
    "        return df_daily1\n",
    "\n",
    "    def z_scores(self):\n",
    "        df_daily = self.mean_revert_trans().copy()\n",
    "        df_daily['Zscore'] = (df_daily['6_month_change'] - df_daily['6_month_change'].mean()) / df_daily['6_month_change'].std()\n",
    "        df_daily = df_daily[~df_daily.index.duplicated(keep='first')]\n",
    "        zscore_df = df_daily.drop([self.field,\"6_month_change\",'6_months_ago'], axis=1)\n",
    "        #df_daily['6_month_change'].apply(stats.zscore)\n",
    "        # Manually calculating Z-scores: z_scores_manual = (df - df.mean()) / df.std()\n",
    "        return zscore_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56a90af-00cc-4e40-b54b-4702118e7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index_that_are_stationary:\n",
    "    #this is the same idea as indexes before, but exlcduing the conversation for stationary if the index is already stationary like a PMI\n",
    "    def __init__(self, bbg_ticker):\n",
    "        #this section allows me to maintian the features throughout the class and instantly lets me pull these features into other classes later wihtout ever defining it. \n",
    "        self.index_value = blp.bdh(tickers=bbg_ticker, flds=['PX_LAST'],start_date='1970-01-01',Per='D', Fill='P', Days='A')\n",
    "        self.ticker = bbg_ticker\n",
    "\n",
    "    def invert_index_value(self):\n",
    "        self.index_value = self.index_value*-1\n",
    "    \n",
    "    def z_scores(self):\n",
    "        self.index_value.index = pd.to_datetime(self.index_value.index)\n",
    "        df_daily = self.index_value[self.ticker].copy()\n",
    "        df_daily['Zscore'] = (df_daily['PX_LAST'] - df_daily['PX_LAST'].mean()) / df_daily['PX_LAST'].std()\n",
    "        df_daily = df_daily[~df_daily.index.duplicated(keep='first')]\n",
    "        zscore_df = df_daily.drop(['PX_LAST'], axis=1)\n",
    "        #df_daily['6_month_change'].apply(stats.zscore)\n",
    "        # Manually calculating Z-scores: z_scores_manual = (df - df.mean()) / df.std()\n",
    "        return zscore_df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638b8ab1-8d44-47ca-bc4b-be53cdf5e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datapoints_that_are_stationary:\n",
    "    def __init__(self, bbg_ticker,field):\n",
    "        #this section allows me to maintian the features throughout the class and instantly lets me pull these features into other classes later wihtout ever defining it. \n",
    "        self.index_value = blp.bdh(tickers=bbg_ticker, flds=field,start_date='1970-01-01',Per='D', Fill='P', Days='A')\n",
    "        self.ticker = bbg_ticker\n",
    "        self.field = field\n",
    "\n",
    "    def invert_index_value(self):\n",
    "        self.index_value = self.index_value*-1\n",
    "        \n",
    "    def z_scores(self):\n",
    "        self.index_value.index = pd.to_datetime(self.index_value.index)\n",
    "        df_daily = self.index_value[self.ticker].copy()\n",
    "        df_daily['Zscore'] = (df_daily[self.field] - df_daily[self.field].mean()) / df_daily[self.field].std()\n",
    "        df_daily = df_daily[~df_daily.index.duplicated(keep='first')]\n",
    "        zscore_df = df_daily.drop([self.field], axis=1)\n",
    "        #df_daily['6_month_change'].apply(stats.zscore)\n",
    "        # Manually calculating Z-scores: z_scores_manual = (df - df.mean()) / df.std()\n",
    "        return zscore_df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49325f4-ecfb-49b2-ab1e-e0c91d67dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HF_Index:\n",
    "    #this class is how we make our headline high frequency indciators, by combining the sub indicies. \n",
    "    def __init__(self):\n",
    "        self.datapoints = [] \n",
    "    \n",
    "    def add_data(self, datapoint):\n",
    "        #this adds the indicators to the class\n",
    "        self.datapoints.append(datapoint)\n",
    "    \n",
    "    def combine_zscores(self):\n",
    "       # this is where we make a dataframe or table of all the z scores from the indicators we've added. \n",
    "        for data in self.datapoints:\n",
    "            zscore_dfsource = data.z_scores() #this loop is just to make the dataframe, starting by getting each sub indicies z scores \n",
    "            zscore_df = zscore_dfsource.copy()\n",
    "            if isinstance(data, Datapoints_that_are_stationary) or isinstance(data, Datapoints_to_stationary): #this if statement checks what class it is in, so I can rename columns and not have duplicates\n",
    "                zscore_df.rename(columns={'Zscore': (data.ticker + \" \" + data.field)}, inplace=True)\n",
    "            else:\n",
    "                zscore_df.rename(columns={'Zscore': data.ticker}, inplace=True)\n",
    "                \n",
    "            if data == self.datapoints[0]:\n",
    "                combined_df = zscore_df.copy()\n",
    "            \n",
    "            else:\n",
    "                combined_df = pd.merge(combined_df,  zscore_df, left_index=True, right_index=True,how='outer')\n",
    "    \n",
    "        \n",
    "        return combined_df \n",
    "    \n",
    "    def highfrequency_index(self, HF_Index_Name_string): #finally this last fucntion takes the average of all the columns fomr the z score data frame and makes the headline index. \n",
    "        hf_df=self.combine_zscores().copy()\n",
    "        hf_df=hf_df.ffill()\n",
    "        hf_df[HF_Index_Name_string] = hf_df.mean(axis=1)\n",
    "        hf_df = hf_df.reset_index()\n",
    "        # Drop duplicates\n",
    "        hf_df = hf_df.drop_duplicates(subset='index')\n",
    "        # Set index back if necessary\n",
    "        hf_df = hf_df.set_index('index')\n",
    "        return hf_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e4ec14-b26d-48ec-8d1d-c4cc3aabbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Housing\n",
    "USHBMIDX=Index_that_are_stationary('USHBMIDX INDEX')\n",
    "NHSPATOT=Index_that_are_stationary('NHSPATOT INDEX')\n",
    "SPCSUSS=Index_to_stationary('SPCSUSS INDEX')\n",
    "CNSTRESI=Index_to_stationary('CNSTRESI INDEX')\n",
    "MBAVBASC=Index_that_are_stationary('MBAVBASC INDEX')\n",
    "\n",
    "housing_index=HF_Index()\n",
    "housing_index.add_data(NHSPATOT)\n",
    "housing_index.add_data(USHBMIDX)\n",
    "housing_index.add_data(SPCSUSS)\n",
    "housing_index.add_data(CNSTRESI)\n",
    "housing_index.add_data(MBAVBASC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87aba060-a959-4d12-91f3-4774ac8509c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Business\n",
    "NAPMALL=Index_that_are_stationary('NAPMALL INDEX')\n",
    "NAPMNEWO=Index_that_are_stationary('NAPMNEWO INDEX')\n",
    "NAPMNNO=Index_that_are_stationary('NAPMNNO INDEX')\n",
    "RCHSINDX=Index_that_are_stationary('RCHSINDX INDEX')\n",
    "RCSSSREV=Index_that_are_stationary('RCSSSREV INDEX')\n",
    "DFEDGBA=Index_that_are_stationary('DFEDGBA INDEX')\n",
    "DSERRVCC=Index_that_are_stationary('DSERRVCC INDEX')\n",
    "EMPRGBCI=Index_that_are_stationary('EMPRGBCI INDEX')\n",
    "CFNAI=Index_that_are_stationary('CFNAI INDEX')\n",
    "MPMIUSCA=Index_that_are_stationary('MPMIUSCA INDEX')\n",
    "OUMFGAF=Index_that_are_stationary('OUMFGAF INDEX')\n",
    "KCLSSACI=Index_that_are_stationary('KCLSSACI INDEX')\n",
    "business_sentiment=[NAPMALL, NAPMNEWO, NAPMNNO, RCHSINDX, RCSSSREV, DFEDGBA, DSERRVCC, EMPRGBCI, CFNAI, MPMIUSCA, OUMFGAF, KCLSSACI]\n",
    "Businesss_index=HF_Index()\n",
    "for i in business_sentiment:\n",
    "    Businesss_index.add_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9205ff63-253a-4a1a-9a49-381448e71734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employment\n",
    "intial_claims =Index_that_are_stationary('INJCJC INDEX')\n",
    "intial_claims.invert_index_value() #inverts the dataframe values of the raw data\n",
    "vacancies  =Index_to_stationary('JOLTPRIV INDEX')\n",
    "HIREPRIR =Index_that_are_stationary('HIREPRIR INDEX')\n",
    "QUITPRIR =Index_that_are_stationary('QUITPRIR INDEX')\n",
    "layoffs =Index_that_are_stationary('CHALTOTL INDEX')\n",
    "layoffs.invert_index_value()\n",
    "household =Index_to_stationary('USLFTOT INDEX')\n",
    "nfp_acyclical  =Index_to_stationary('USEETOTS INDEX')\n",
    "USURTOT =Index_that_are_stationary('USURTOT INDEX')\n",
    "USURTOT.invert_index_value()\n",
    "AWH_TOTL =Index_that_are_stationary('AWH TOTL INDEX')\n",
    "NAPMNEMP =Index_that_are_stationary('NAPMNEMP INDEX')\n",
    "NAPMEMPL =Index_that_are_stationary('NAPMEMPL INDEX')\n",
    "employment_list=[intial_claims, vacancies, HIREPRIR, QUITPRIR, layoffs, household, nfp_acyclical, USURTOT, AWH_TOTL, NAPMNEMP, NAPMEMPL]\n",
    "Employment_index=HF_Index()\n",
    "for i in employment_list:\n",
    "    Employment_index.add_data(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b5b24f-545e-4790-81a5-2831bb5457b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Profits\n",
    "spx_trail_eps=Datapoints_to_stationary(\"SPX INDEX\", \"TRAIL_12M_EPS\")\n",
    "spx_best_eps=Datapoints_to_stationary(\"SPX INDEX\", \"BEST_EPS\")\n",
    "spx_margin=Datapoints_to_stationary(\"SPX INDEX\", \"TRAIL_12M_PROF_MARGIN\")\n",
    "spx_sales=Datapoints_to_stationary(\"SPX INDEX\", \"BEST_NI_ADJ_TO_SALES\")\n",
    "\n",
    "mid_trail_eps=Datapoints_to_stationary(\"MID INDEX\", \"TRAIL_12M_EPS\")\n",
    "mid_best_eps=Datapoints_to_stationary(\"MID INDEX\", \"BEST_EPS\")\n",
    "mid_margin=Datapoints_to_stationary(\"MID INDEX\", \"TRAIL_12M_PROF_MARGIN\")\n",
    "mid_sales=Datapoints_to_stationary(\"MID INDEX\", \"BEST_NI_ADJ_TO_SALES\")\n",
    "\n",
    "sml_trail_eps=Datapoints_to_stationary(\"SML INDEX\", \"TRAIL_12M_EPS\")\n",
    "sml_best_eps=Datapoints_to_stationary(\"SML INDEX\", \"BEST_EPS\")\n",
    "sml_margin=Datapoints_to_stationary(\"SML INDEX\", \"TRAIL_12M_PROF_MARGIN\")\n",
    "sml_sales=Datapoints_to_stationary(\"SML INDEX\", \"BEST_NI_ADJ_TO_SALES\")\n",
    "\n",
    "bm7t_trail_eps=Datapoints_to_stationary(\"BM7T INDEX\", \"TRAIL_12M_EPS\")\n",
    "bm7t_best_eps=Datapoints_to_stationary(\"BM7T INDEX\", \"BEST_EPS\")\n",
    "bm7t_margin=Datapoints_to_stationary(\"BM7T INDEX\", \"TRAIL_12M_PROF_MARGIN\")\n",
    "bm7t_sales=Datapoints_to_stationary(\"BM7T INDEX\", \"BEST_NI_ADJ_TO_SALES\")\n",
    "\n",
    "b500xm7t_trail_eps=Datapoints_to_stationary(\"B500XM7T INDEX\", \"TRAIL_12M_EPS\")\n",
    "b500xm7t_best_eps=Datapoints_to_stationary(\"B500XM7T INDEX\", \"BEST_EPS\")\n",
    "b500xm7t_margin=Datapoints_to_stationary(\"B500XM7T INDEX\", \"TRAIL_12M_PROF_MARGIN\")\n",
    "b500xm7t_sales=Datapoints_to_stationary(\"B500XM7T INDEX\", \"BEST_NI_ADJ_TO_SALES\")\n",
    "\n",
    "GVADNCPA =Index_that_are_stationary('GVADNCPA INDEX')\n",
    "\n",
    "profits_list = [spx_trail_eps,spx_best_eps,spx_margin,spx_sales,mid_trail_eps,mid_best_eps,mid_margin,mid_sales,sml_trail_eps,sml_margin,sml_sales,bm7t_trail_eps,bm7t_best_eps,bm7t_margin,bm7t_sales,b500xm7t_trail_eps,b500xm7t_best_eps,b500xm7t_margin,b500xm7t_sales]\n",
    "\n",
    "Profits_index=HF_Index()\n",
    "for i in profits_list:\n",
    "    Profits_index.add_data(i)  \n",
    "Profits_index.add_data(GVADNCPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94e195f0-2e48-4426-a77f-fc105e9a2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit\n",
    "TDBCTOTL =Index_to_stationary('TDBCTOTL INDEX')\n",
    "ALBNLOAN = Index_to_stationary('ALBNLOAN INDEX')\n",
    "ALBNC_IL = Index_to_stationary('ALBNC&IL INDEX')\n",
    "ALBNLLCN = Index_to_stationary('ALBNLLCN INDEX')\n",
    "SLDETIGT = Index_to_stationary('SLDETIGT INDEX')\n",
    "NFCINBON =Index_that_are_stationary('NFCINBON INDEX')\n",
    "SBOIEXCR =Index_that_are_stationary('SBOIEXCR INDEX')\n",
    "SBOILOAN =Index_that_are_stationary('SBOILOAN INDEX')\n",
    "\n",
    "credit_list = [TDBCTOTL, ALBNLOAN,ALBNC_IL,ALBNLLCN ,SLDETIGT ,NFCINBON ,SBOIEXCR ,SBOILOAN] \n",
    "\n",
    "Credit_index=HF_Index()\n",
    "for i in credit_list:\n",
    "    Credit_index.add_data(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b9b196-3a05-48e6-b20c-192b1dccf6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consumer\n",
    "\n",
    "CONSSENT =Index_that_are_stationary('CONSSENT INDEX')\n",
    "USDECNLN =Index_that_are_stationary('USDECNLN INDEX')\n",
    "CONCEXP =Index_that_are_stationary('CONCEXP INDEX')\n",
    "MTSLRRT =Index_to_stationary('MTSLRRT$ INDEX')\n",
    "CPI_YOY =Index_that_are_stationary('CPI YOY Index')\n",
    "REDSMYOY =Index_that_are_stationary('REDSMYOY INDEX')\n",
    "\n",
    "REDSMYOY_copy=REDSMYOY.index_value[REDSMYOY.ticker].copy() # here I need to rewrite the index data as my clas sisn't flexile enough to address changes like real retail sales\n",
    "CPI_YOY_copy=CPI_YOY.index_value[CPI_YOY.ticker].copy()\n",
    "REDSMYOY_copy=pd.merge(REDSMYOY_copy, CPI_YOY_copy, left_index=True, right_index=True)\n",
    "REDSMYOY_copy['Real']=REDSMYOY_copy['PX_LAST_x']- REDSMYOY_copy['PX_LAST_y'] #here I am subtracting teh change in y/y retail sales by CPI y/y to get real retail sales y/y\n",
    "REDSMYOY.index_value[REDSMYOY.ticker] = REDSMYOY_copy['Real']\n",
    "\n",
    "PCE_CONC =Index_to_stationary('PCE CONC INDEX')\n",
    "\n",
    "PCE_CONC_trend =Index_that_are_stationary('PCE CONC INDEX') #this process is me getting the trend in PCE consumption by a simple regression on time and them editing the raw data to be process into sationary\n",
    "model = LinearRegression()\n",
    "x = np.arange(len(PCE_CONC_trend.index_value.dropna().index)).reshape(-1, 1)\n",
    "y = PCE_CONC_trend.index_value.dropna().values\n",
    "model.fit(x, y)\n",
    "# Extract the trend\n",
    "PCE_CONC_trend_copy=PCE_CONC_trend.index_value[PCE_CONC_trend.ticker].copy().dropna()\n",
    "PCE_CONC_trend_copy[\"Trend\"] = model.predict(x)\n",
    "PCE_CONC_trend_copy['Deviation from Trend'] = PCE_CONC_trend_copy[\"PX_LAST\"] - PCE_CONC_trend_copy[\"Trend\"]\n",
    "PCE_CONC_trend.index_value[PCE_CONC_trend.ticker] = PCE_CONC_trend_copy['Deviation from Trend']\n",
    "\n",
    "consumer_list = [CONSSENT,CONCEXP,USDECNLN,MTSLRRT,REDSMYOY ,PCE_CONC ,PCE_CONC_trend] \n",
    "\n",
    "Consumer_index=HF_Index()\n",
    "for i in consumer_list:\n",
    "    Consumer_index.add_data(i) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "425936b0-fc0e-41b3-bd5a-53b4809fc830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fiscal\n",
    "gov_expend =Index_to_stationary('ECOGBUSN INDEX') #fiscal index is % of GDP so I had to alter the raw data as well for teh classes to work. \n",
    "gdp =Index_to_stationary('GDP CUR$ INDEX')\n",
    "fiscal = Index_that_are_stationary('ECOGBUSN INDEX')\n",
    "gov_expend_copy=gov_expend.index_value[gov_expend.ticker].copy()\n",
    "gdp_copy=gdp.index_value[gdp.ticker].copy()\n",
    "fiscal_deficit = gov_expend_copy/gdp_copy\n",
    "gov_expend.index_value[gov_expend.ticker] = fiscal_deficit\n",
    "fiscal.index_value[fiscal.ticker]=gov_expend.mean_revert_trans()[\"PX_LAST\"]-gov_expend.mean_revert_trans()[\"6_months_ago\"]\n",
    "fiscal.invert_index_value()\n",
    "\n",
    "Fiscal_index=HF_Index()\n",
    "Fiscal_index.add_data(fiscal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60b9985e-2f77-4f8d-ad89-0fe21943ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geo Politics\n",
    "geo_politics = Haver.data('DGLGPRD', database='ESG', startdate='1970-01-01',frequency = 'caldaily') #Geopoltics is ahver data, but I wrote the class for Bloomberg, so I'm just changing the data to match bloomberg\n",
    "geo_politics=geo_politics.to_timestamp().reset_index()\n",
    "days_between = (pd.to_datetime('today').normalize() - geo_politics[\"index\"].iloc[-1]).days\n",
    "day_between_market = int(days_between-(days_between/7*2))\n",
    "geo_politics[\"index\"] = geo_politics[\"index\"] + timedelta(days=(pd.to_datetime('today').normalize() - geo_politics[\"index\"].iloc[-1]).days)\n",
    "geo_politics=geo_politics.set_index(\"index\")\n",
    "geo_politics=geo_politics.shift(days_between*-1)\n",
    "Geo_Politics =Index_that_are_stationary('APEZCOGR Index')\n",
    "Geo_Politics_copy=Geo_Politics.index_value[Geo_Politics.ticker].copy()\n",
    "Geo_Politics_copy=pd.merge(Geo_Politics_copy, geo_politics, left_index=True, right_index=True)\n",
    "Geo_Politics.index_value[Geo_Politics.ticker] = Geo_Politics_copy['dglgprd']\n",
    "\n",
    "GeoPolitical_index=HF_Index()\n",
    "GeoPolitical_index.add_data(Geo_Politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c9d7426-169e-413a-b158-af2718874e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delinquencies/Bankruptcies\n",
    "USEQTATO =Index_that_are_stationary('USEQTATO INDEX')\n",
    "USEQTLTO =Index_that_are_stationary('USEQTLTO INDEX')\n",
    "USEQCCTO =Index_that_are_stationary('USEQCCTO INDEX')\n",
    "BNKRINDX =Index_that_are_stationary('BNKRINDX INDEX')\n",
    "\n",
    "delinquencies_list=[USEQTATO, USEQTLTO, USEQCCTO, BNKRINDX]\n",
    "Delinquencies_Bankruptcies_index=HF_Index()\n",
    "for i in delinquencies_list:\n",
    "    Delinquencies_Bankruptcies_index.add_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2b56c04-3011-4669-862d-d6402cd7f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inflation\n",
    "NAPMPRIC =Index_that_are_stationary('NAPMPRIC INDEX')\n",
    "NAPMNPRC =Index_that_are_stationary('NAPMNPRC INDEX')\n",
    "FRGHGLOB =Index_to_stationary('FRGHGLOB INDEX')\n",
    "PCE_DEF =Index_to_stationary('PCE DEF INDEX')\n",
    "FDIUFDA =Index_to_stationary('FDIUFDA INDEX')\n",
    "FDIUSGA =Index_to_stationary('FDIUSGA INDEX')\n",
    "AHE_TOTL =Index_to_stationary('AHE TOTL INDEX')\n",
    "AWP_TOTL =Index_to_stationary('AWP TOTL INDEX')\n",
    "ECICCIVL =Index_to_stationary('ECICCIVL INDEX')\n",
    "BCOMEN =Index_to_stationary('BCOMEN INDEX')\n",
    "BCOMIN =Index_to_stationary('BCOMIN INDEX')\n",
    "BCOMAG =Index_to_stationary('BCOMAG INDEX')\n",
    "COSTNFRM =Index_to_stationary('COSTNFRM INDEX')\n",
    "\n",
    "inflation_list=[NAPMPRIC, NAPMNPRC, FRGHGLOB, PCE_DEF, NAPMPRIC,NAPMNPRC, FRGHGLOB, PCE_DEF, FDIUFDA, FDIUSGA, AHE_TOTL, AWP_TOTL, ECICCIVL, BCOMEN, BCOMIN, BCOMAG, COSTNFRM ]\n",
    "Inflation_index=HF_Index()\n",
    "for i in inflation_list:\n",
    "    Inflation_index.add_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6d9de1b-0fc2-42f0-9872-ba2eaf5914cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Financial conditions\n",
    "GSUSFCI =Index_that_are_stationary('GSUSFCI INDEX')\n",
    "BICLB10Y =Index_that_are_stationary('BICLB10Y INDEX')\n",
    "long_yields =Index_that_are_stationary('USGG10YR INDEX')\n",
    "CPI_INDX =Index_to_stationary('CPI INDX INDEX')\n",
    "fed =Index_that_are_stationary('FDTR INDEX')\n",
    "BFCIUS =Index_that_are_stationary('BFCIUS INDEX')\n",
    "BFCIUS.invert_index_value()\n",
    "NFCIINDX =Index_that_are_stationary('NFCIINDX INDEX')\n",
    "KCFSINDX =Index_that_are_stationary('KCFSINDX INDEX')\n",
    "FRNYCMDI =Index_that_are_stationary('FRNYCMDI INDEX')\n",
    "FCIGMB1Y =Index_that_are_stationary('FCIGMB1Y INDEX')\n",
    "USNRUS =Index_that_are_stationary('USNRUS INDEX')\n",
    "FCIGMB3Y =Index_that_are_stationary('FCIGMB3Y INDEX')\n",
    "\n",
    "long_yields_copy=long_yields.index_value[long_yields.ticker].copy() #here i'm getting the changes in long yields by taking away the natural rate of interest and inflation.  So growth expectations\n",
    "CPI_INDX_copy=CPI_INDX.mean_revert_trans()[['6_month_change']].copy()*100\n",
    "USNRUS_copy=USNRUS.index_value[USNRUS.ticker].copy()\n",
    "long_yields_copy=pd.merge(long_yields_copy, USNRUS_copy, left_index=True, right_index=True)\n",
    "long_yields_copy=pd.merge(long_yields_copy, CPI_INDX_copy, left_index=True, right_index=True)\n",
    "long_yields_copy['long_yields']=long_yields_copy['PX_LAST_x']- long_yields_copy['PX_LAST_y']-long_yields_copy['6_month_change']\n",
    "long_yields.index_value[long_yields.ticker] = long_yields_copy['long_yields']\n",
    "\n",
    "fed_copy = fed.index_value[fed.ticker].copy() #here I'm altering the rw data to relfect the restcitivness of real policy rate\n",
    "fed_copy=pd.merge(fed_copy, USNRUS_copy, left_index=True, right_index=True)\n",
    "fed_copy=pd.merge(fed_copy, CPI_INDX_copy, left_index=True, right_index=True)\n",
    "fed_copy['long_yields']=fed_copy['PX_LAST_x']- fed_copy['PX_LAST_y']-fed_copy['6_month_change']\n",
    "fed.index_value[fed.ticker] = fed_copy['long_yields']\n",
    "\n",
    "fc_list=[GSUSFCI, \tBICLB10Y, \tlong_yields, fed, \tBFCIUS, \tNFCIINDX, \tKCFSINDX, \tFRNYCMDI, \tFCIGMB3Y ,FCIGMB1Y ]\n",
    "Financial_conditions_index=HF_Index()\n",
    "for i in fc_list:\n",
    "    Financial_conditions_index.add_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bcefb33-2df6-4e75-be24-eca597e26e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Market Bubble Risk\n",
    "\n",
    "SPX_PE=Datapoints_that_are_stationary(\"SPX Index\", \"PE_RATIO\")\n",
    "CHG_SPX_PE=Datapoints_to_stationary(\"SPX Index\", \"PE_RATIO\")\n",
    "SPX_eps=Datapoints_to_stationary(\"SPX INDEX\", \"TRAIL_12M_EPS\")\n",
    "real_spx_eps=Datapoints_to_stationary(\"SPX INDEX\", \"TRAIL_12M_EPS\")\n",
    "fundamental=Datapoints_that_are_stationary(\"SPX INDEX\", \"TRAIL_12M_EPS\")\n",
    "CPI_INDX =Index_to_stationary('CPI INDX INDEX')\n",
    "SPX =Index_to_stationary('SPX INDEX')\n",
    "concentration =Index_to_stationary('SPX INDEX')\n",
    "real_spx =Index_to_stationary('SPX INDEX')\n",
    "SPW =Index_to_stationary('SPW INDEX')\n",
    "AAA =Index_that_are_stationary('BASPCAAA INDEX')\n",
    "BBB =Index_that_are_stationary('CSI BBB INDEX')\n",
    "CCC =Index_that_are_stationary('CSI BARC INDEX')\n",
    "\n",
    "real_price = real_spx.index_value[real_spx.ticker].copy() #here i'm replacing the SPX index with an inflation adjusted one\n",
    "CPI_INDX_copy=CPI_INDX.index_value[CPI_INDX.ticker].copy()\n",
    "real_price = pd.merge(real_price, CPI_INDX_copy, left_index=True, right_index=True)\n",
    "real_price['Real SPX']=real_price['PX_LAST_x']/real_price['PX_LAST_y']\n",
    "real_spx.index_value[real_spx.ticker] = real_price['Real SPX']\n",
    "\n",
    "real_eps = real_spx_eps.index_value[real_spx_eps.ticker].copy() #here I'm replacing eps with real eps\n",
    "real_eps = pd.merge(real_eps, CPI_INDX_copy, left_index=True, right_index=True)\n",
    "real_eps['Real EPS']=real_eps['TRAIL_12M_EPS']/real_eps['PX_LAST']\n",
    "real_spx_eps.index_value[real_spx.ticker] = real_eps['Real EPS']*100\n",
    "\n",
    "SPX_eps_copy=SPX_eps.mean_revert_trans()[['6_month_change']].copy()*100 # I'm making a varaible called fudnamnetal, which \n",
    "SPX_eps_copy_abs = SPX_eps_copy.copy().abs()\n",
    "CHG_SPX_PE_copy=CHG_SPX_PE.mean_revert_trans()[['6_month_change']].copy()*100\n",
    "CHG_SPX_PE_copy = CHG_SPX_PE_copy.abs()\n",
    "SPX_eps_copy = pd.merge(SPX_eps_copy, SPX_eps_copy, left_index=True, right_index=True)\n",
    "SPX_eps_copy = pd.merge(SPX_eps_copy, CHG_SPX_PE_copy, left_index=True, right_index=True)\n",
    "SPX_eps_copy['Fundamental'] = (SPX_eps_copy['6_month_change_x']/(SPX_eps_copy['6_month_change_y']+SPX_eps_copy['6_month_change']))*-1\n",
    "fundamental.index_value[fundamental.ticker] = SPX_eps_copy['Fundamental']\n",
    "\n",
    "concentration_copy = concentration.mean_revert_trans()[['6_month_change']].copy()*100 #this measures market concentration \n",
    "SPW_copy=SPW.mean_revert_trans()[['6_month_change']].copy()*100\n",
    "concentration_copy = pd.merge(concentration_copy, SPW_copy, left_index=True, right_index=True)\n",
    "concentration_copy['Concentration']=concentration_copy['6_month_change_x']-concentration_copy['6_month_change_y']\n",
    "concentration.index_value[concentration.ticker] = concentration_copy['Concentration']\n",
    "\n",
    "bubble_list=[SPX_PE, \tfundamental, \tconcentration, AAA, \tBBB, \tCCC]\n",
    "Bubble_index=HF_Index()\n",
    "for i in bubble_list:\n",
    "    Bubble_index.add_data(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb5abb1a-0ca8-4788-a7e5-d943c49b2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_daily(df): #this is code aimed at converting a mnthly frequency to daily, needed for haver data\n",
    "    df_daily = df.copy()\n",
    "    df_daily = df_daily.to_timestamp()\n",
    "    df_daily.index = df_daily.index.to_period('M').to_timestamp('D')\n",
    "    #df_daily = df_daily.resample('D').ffill()\n",
    "    return df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ccd90a7-10c4-4951-b104-b1ee2ec4c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECAPE = Haver.data('SPECAPE', database='USECON', startdate='1970-01-01') #this is all work to get eh Cyclcially adjusted PE ratio from haver to daily\n",
    "CAPE=Datapoints_that_are_stationary(\"SPX Index\", \"PE_RATIO\")\n",
    "specape_daily=convert_to_daily(SPECAPE).reset_index()\n",
    "specape_daily[\"index\"] = specape_daily[\"index\"] + timedelta(days=(pd.to_datetime('today').normalize() - specape_daily[\"index\"].iloc[-1]).days)\n",
    "specape_daily=specape_daily.set_index(\"index\")\n",
    "specape_daily = specape_daily.resample('D').ffill()\n",
    "CAPE.index_value[CAPE.ticker] = specape_daily['specape']\n",
    "Bubble_index.add_data(CAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9844234c-2b8c-4819-9932-54e9dce4fa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limacinnis\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "#finally we bring all the indicators together to create the table of headline indicies \n",
    "housing_index_indicator=housing_index.highfrequency_index(\"Housing Index\")\n",
    "Businesss_index_indicator=Businesss_index.highfrequency_index(\"Business Index\")\n",
    "Employment_index_indicator=Employment_index.highfrequency_index(\"Employment Index\")\n",
    "Profits_index_indicator=Profits_index.highfrequency_index(\"Profits Index\")\n",
    "Credit_index_indicator=Credit_index.highfrequency_index(\"Credit Index\")\n",
    "Consumer_index_indicator=Consumer_index.highfrequency_index(\"Consumer Index\")\n",
    "Fiscal_index_indicator=Fiscal_index.highfrequency_index(\"Fiscal Index\")\n",
    "GeoPolitical_index_indicator=GeoPolitical_index.highfrequency_index(\"Geo Politics Index\")\n",
    "Delinquencies_Bankruptcies_index_indicator=Delinquencies_Bankruptcies_index.highfrequency_index(\"Solvency Index\")\n",
    "Inflation_index_indicator=Inflation_index.highfrequency_index(\"Inflation Index\")\n",
    "Financial_conditions_index_indicator=Financial_conditions_index.highfrequency_index(\"Financial Conditions Index\")\n",
    "Bubble_index_indicator=Bubble_index.highfrequency_index(\"Bubble Index\")\n",
    "bubble_cols = [\"S&P PE\", \t\"Fundamental\", \t\"Concentration\", \"AAA Spreads\", \t\"BBB Spread\", \t\"CCC Spreads\",\"CAPE\", \"Bubble Index\"]\n",
    "Bubble_index_indicator.columns = bubble_cols\n",
    "\n",
    "HF_indicators_df = housing_index_indicator.iloc[:,-1:] #the last column of each indictor dataframe is the headline indicator. \n",
    "\n",
    "index_list = [Businesss_index_indicator,Employment_index_indicator,Profits_index_indicator,Credit_index_indicator,Consumer_index_indicator,Fiscal_index_indicator,\n",
    "              GeoPolitical_index_indicator,Delinquencies_Bankruptcies_index_indicator,Inflation_index_indicator,Financial_conditions_index_indicator,Bubble_index_indicator]\n",
    "for i in index_list:\n",
    "    HF_indicators_df = pd.merge(HF_indicators_df, i.iloc[:,-1:], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd08702-56bd-47fb-8422-1788f2cbbabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
